#FIRST PART: THE CODE OF THE TRAINING THE MODEL AND SAVING IT
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import regularizers
import json
import pickle
import matplotlib.pyplot as plt
from tabulate import tabulate
from IPython.display import display, HTML
from tensorflow.keras.callbacks import TensorBoard
import datetime

# CLASS THAT GIVES COLORS TO TEXTS
class ColorPrinter:

    @staticmethod
    def print_red(text):
        return f"<span style='color:red; font-size: 16px;'>{text}</span>"

    @staticmethod
    def print_orange(text):
        return f"<span style='color:orange; font-size: 16px;'>{text}</span>"

    @staticmethod
    def print_green(text):
        return f"<span style='color:green; font-size: 16px;'>{text}</span>"

    @staticmethod
    def print_yellow(text):
        return f"<span style='color:yellow; font-size: 16px;'>{text}</span>"

    @staticmethod
    def print_blue(text):
        return f"<span style='color:blue; font-size: 16px;'>{text}</span>"

    @staticmethod
    def print_magenta(text):
        return f"<span style='color:magenta; font-size: 16px;'>{text}</span>"

    @staticmethod
    def print_cyan(text):
        return f"<span style='color:cyan; font-size: 16px;'>{text}</span>"


# CLASS THAT CONTAINS THE PROCESSES ABOUT PREPARING THE DATA TO CREATE A MODEL
class DataProcessor:

    def __init__(self, file_path_signal, file_path_background):
        self.file_path_signal = file_path_signal
        self.file_path_background = file_path_background
        self.df_balanced = None
        self.color_printer = ColorPrinter()

    def load_data(self):
        df_signal = pd.read_csv(self.file_path_signal)
        df_background = pd.read_csv(self.file_path_background)
        df_background.iloc[1:] = df_background.iloc[1:].sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffling background
        self.df = pd.concat([df_signal, df_background])
        return self.df

    def balance_data(self):
        class_counts = self.df['DATA_MC_CHECK'].value_counts()
        minority_class_label = class_counts.idxmin()
        majority_class_label = class_counts.idxmax()

        minority_class = self.df[self.df['DATA_MC_CHECK'] == minority_class_label]
        majority_class = self.df[self.df['DATA_MC_CHECK'] == majority_class_label]

        majority_class_downsampled = resample(majority_class,
                                              replace=False,
                                              n_samples=len(minority_class),
                                              random_state=42)

        self.df_balanced = pd.concat([minority_class, majority_class_downsampled])
        self.df_balanced = self.df_balanced.sample(frac=1, random_state=79).reset_index(drop=True)

        display(HTML(self.color_printer.print_yellow("Balanced DataFrame row count: {}".format(len(self.df_balanced)))))
        display(HTML(self.color_printer.print_yellow("<br><br>Sample rows from balanced DataFrame: <br>")))
        print(tabulate(self.df_balanced.sample(7), headers='keys', tablefmt='psql'))

        return self.df_balanced

    def preprocess_data(self):
        X = self.df_balanced.drop('DATA_MC_CHECK', axis=1)
        y = self.df_balanced['DATA_MC_CHECK']

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        with open('scaler.pkl', 'wb') as f:  # Saving the StandardScaler object to use it in the new-data-loading part.
            pickle.dump(scaler, f)

        return X_train, X_test, y_train, y_test

# CLASS THAT BUILDS THE MODEL
class ModelBuilder:
    def __init__(self):
        self.model = None

    def build_model(self):
        self.model = tf.keras.Sequential([
            tf.keras.layers.Dense(12, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
            tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
            tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])

        self.model.compile(
            loss=tf.keras.losses.BinaryCrossentropy(),
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]
        )

        return self.model

# UPLOADING THE DATASETS
file_path_signal = "/content/drive/MyDrive/analizando/ZPrime_shuffled.csv"
file_path_background = "/content/drive/MyDrive/analizando/background.csv"

# DATA PROCESSING
data_processor = DataProcessor(file_path_signal, file_path_background)
df = data_processor.load_data()
df_balanced = data_processor.balance_data()
X_train, X_test, y_train, y_test = data_processor.preprocess_data()

# MODEL BUILDING
model_builder = ModelBuilder()
model = model_builder.build_model()
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Create folders for TensorBoard log files
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

""" 
%load_ext tensorboard
%tensorboard --logdir logs/fit

Run this code in another cell to access the TensorBoard to analyze the training process in detail.
"""

# TRAINING THE MODEL
display(HTML(ColorPrinter.print_yellow("<br><br>TRAINING THE MODEL:<br><br>")))

history = model.fit(X_train, y_train, epochs=100, batch_size=1024, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, early_stopping])

model.save('ZPrime_prediction_model.h5')


# *** PERFORMANCE METRICS INTRODUCTION ***

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix, precision_recall_curve, matthews_corrcoef, log_loss

y_pred = model.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype("int32")

# Defining the metrics
accuracy = accuracy_score(y_test, y_pred_classes)
f1 = f1_score(y_test, y_pred_classes)
roc_auc = roc_auc_score(y_test, y_pred)
precision = precision_score(y_test, y_pred_classes)
recall = recall_score(y_test, y_pred_classes)
conf_matrix = confusion_matrix(y_test, y_pred_classes)
mcc = matthews_corrcoef(y_test, y_pred_classes)
logloss = log_loss(y_test, y_pred)

# Plotting ROC curve
def plot_roc_curve(y_test, y_pred):
    from sklearn.metrics import roc_curve
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)
    plt.plot(fpr, tpr, linestyle='--', label='ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()

# Evaluation of metrics
def evaluate_metric(metric_name, value):
    if metric_name in ["Accuracy", "F1 Score", "Precision", "Recall"]:
        if value < 0.60:
            return "Bad"
        elif value < 0.75:
            return "Good enough"
        elif value < 0.85:
            return "Good"
        elif value < 0.90:
            return "Very good"
        else:
            return "Extremely good"
    elif metric_name == "ROC AUC Score":
        if value < 0.65:
            return "Bad"
        elif value < 0.75:
            return "Good enough"
        elif value < 0.85:
            return "Good"
        elif value < 0.90:
            return "Very good"
        else:
            return "Extremely good"
    elif metric_name == "Log Loss":
        if value > 1.0:
            return "Bad"
        elif value > 0.5:
            return "Good enough"
        elif value > 0.2:
            return "Good"
        elif value > 0.1:
            return "Very good"
        else:
            return "Extremely good"
    elif metric_name == "Matthews Correlation Coefficient":
        if value < 0.0:
            return "Bad"
        elif value < 0.30:
            return "Good enough"
        elif value < 0.60:
            return "Good"
        elif value < 0.85:
            return "Very good"
        else:
            return "Extremely good"
    return "Unknown"


# THE PROCESS OF PERFORMANCE METRICS
display(HTML(ColorPrinter.print_yellow("<br><br>PERFORMANCE METRICS:<br><br>")))


metrics = {
    "Accuracy": accuracy,
    "F1 Score": f1,
    "ROC AUC Score": roc_auc,
    "Precision": precision,
    "Recall": recall,
    "Matthews Correlation Coefficient": mcc,
    "Log Loss": logloss
}

for metric_name, value in metrics.items():
    evaluation = evaluate_metric(metric_name, value)
    if evaluation == "Bad":
        display(HTML(f"{metric_name}: {value:.10f} - {ColorPrinter.print_red(evaluation)}"))
    elif evaluation == "Good enough":
        display(HTML(f"{metric_name}: {value:.10f} - {ColorPrinter.print_magenta(evaluation)}"))
    elif evaluation == "Good":
        display(HTML(f"{metric_name}: {value:.10f} - {ColorPrinter.print_cyan(evaluation)}"))
    elif evaluation == "Very good":
        display(HTML(f"{metric_name}: {value:.10f} - {ColorPrinter.print_yellow(evaluation)}"))
    elif evaluation == "Extremely good":
        display(HTML(f"{metric_name}: {value:.10f} - {ColorPrinter.print_green(evaluation)}"))

# EXPLANATION OF METRICS ABOVE
display(HTML(ColorPrinter.print_yellow("<br>Explanations of above metrics:<br>")))
display(HTML("<br><strong>Accuracy:</strong> Measures the overall correctness of the model's predictions. It is the ratio of correctly predicted instances to the total instances. High accuracy in particle physics ensures reliable identification of particle events."))
display(HTML("<br><strong>F1 Score:</strong> The harmonic mean of precision and recall. It balances the trade-off between precision and recall, making it a useful metric for evaluating the performance of classifiers, especially on imbalanced datasets commonly found in particle physics."))
display(HTML("<br><strong>ROC AUC Score:</strong> Represents the area under the Receiver Operating Characteristic curve. A higher AUC indicates better model performance in distinguishing between the positive and negative classes. In particle physics, a high AUC score is crucial for distinguishing between signal and background events."))
display(HTML("<br><strong>Precision:</strong> The ratio of true positive predictions to the total predicted positives. High precision means that the classifier has a low false positive rate, which is vital in particle physics to ensure that identified particle events are actually signal events."))
display(HTML("<br><strong>Recall:</strong> The ratio of true positive predictions to the actual positives. High recall ensures that most of the actual signal events are identified by the model, which is critical in experiments where missing a signal event can be costly."))
display(HTML("<br><strong>Matthews Correlation Coefficient:</strong> A measure of the quality of binary classifications. It takes into account true and false positives and negatives, and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. High MCC indicates a strong correlation between the predicted and actual classes, which is essential in particle physics for reliable event classification."))
display(HTML("<br><strong>Log Loss:</strong> Measures the performance of a classification model where the prediction input is a probability value between 0 and 1. Lower log loss indicates better performance. In particle physics, minimizing log loss helps in achieving more accurate probabilistic predictions for particle events, improving overall model reliability."))

# CONFUSION MATRIX
display(HTML(ColorPrinter.print_orange("<br>CONFUSION MATRIX:<br>")))
print(conf_matrix)
display(HTML("<br><strong>Confusion Matrix:</strong> A table used to evaluate the performance of a classification model by comparing the actual and predicted classifications. It shows the counts of true positive, true negative, false positive, and false negative predictions. In particle physics, the confusion matrix helps in understanding the types of errors the model makes and how well it can distinguish between different classes of particle events."))

display(HTML(ColorPrinter.print_yellow("<br><br>GRAPHS:<br>")))

# ROC CURVE PLOTTING
display(HTML(ColorPrinter.print_orange("<br><br>ROC CURVE:<br><br>")))
print("The ROC curve, or Receiver Operating Characteristic curve, is a graphical representation of a classifier's performance. It plots the True Positive Rate (Sensitivity) against the False Positive Rate (1 - Specificity) at various threshold settings. In particle physics, this curve helps in understanding the trade-off between detecting true signal events and the rate of false alarms. A perfect classifier has an area under the ROC curve (AUC) of 1, indicating it perfectly distinguishes between signal and background. \n")
plot_roc_curve(y_test, y_pred)

# DEFINING THE LOSS OF TRAIN AND TEST
loss = history.history['loss']
val_loss = history.history['val_loss']

# DEFINING THE ACCURACY OF TRAIN AND TEST
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

epochs = range(1, len(loss) + 1)

# THE GRAPH OF TRAINING AND VALIDATION LOSS
display(HTML(ColorPrinter.print_orange("<br><br>TRAINING AND VALIDATION LOSS:<br><br>")))
print("The Training and Validation Loss graphs illustrate the model's performance over the training epochs. The loss function quantifies the difference between the predicted and true values. In particle physics experiments, minimizing the loss function is crucial for improving the accuracy of particle identification models. A decreasing training loss indicates that the model is learning from the data, while the validation loss helps in monitoring overfitting.\n")
plt.figure(figsize=(12, 5))
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# THE GRAPH OF TRAINING AND VALIDATION ACCURACY
display(HTML(ColorPrinter.print_orange("<br><br>TRAINING AND VALIDATION ACCURACY:<br><br>")))
print("The Training and Validation Accuracy graphs show how well the model's predictions match the actual data over the epochs. High accuracy means the model is correctly identifying signal and background events. In the context of particle physics, high accuracy ensures that we can trust the model's predictions when identifying rare particle interactions or decays.\n")
plt.figure(figsize=(12, 5))
plt.plot(epochs, train_accuracy, 'b', label='Training accuracy')
plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# PRECISION-RECALL CURVE PLOT
display(HTML(ColorPrinter.print_orange("<br><br>PRECISION-RECALL CURVE:<br><br>")))
print("The Precision-Recall curve is crucial for evaluating the performance of classifiers on imbalanced datasets. Precision indicates the fraction of true positive predictions among all positive predictions, while recall measures the fraction of true positives identified among all actual positives. In particle physics, high precision and recall are vital for ensuring that rare particle events are correctly identified without being overwhelmed by false positives.\n")
precision, recall, _ = precision_recall_curve(y_test, y_pred)
plt.figure()
plt.plot(recall, precision, marker='.', label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall curve')
plt.legend()
plt.show()



